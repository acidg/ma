% !TeX root = ../main.tex

\chapter{Future Work}\label{chapter:future_work}
The approach proofed to be able to find copied code and licensing issues.
It is scalable and can be heavily distributed, which may even make it possible to index and track the history of most of the open source code available on the Internet.
Clients can quickly search their codebase for copied code using the Bloom filter, which can be downloaded from the server.
However, there is a lot of room for improvement:

\section{Distributed Server Architecture}
Since this work proposes a client-server architecture, it is important to provide a reliable service to a huge amount of clients.
Therefor, distributing the index on several machines is desirable for load balancing.
This could easily be done by distributing the database onto the machines whenever the index is updated.

It may also be advisable to group reference projects by license or other parameters like origin of the code (GitHub, Bitbucket, Debian source repositories, \dots).
Pairs of servers and filters responsible for a group of reference projects could be created.
This would result in multiple small hash filters and enables clients to only choose filters of their interest, further reducing the size of the filter a client has to download and keep up to date.
The requests resulting from matches in the filter on the client side then have to be routed to the server responsible for the corresponding group of reference projects.

Distributing the workload can also speed up the indexing process, when updates are triggered (see \autoref{section:implementation/scalability}).

\section{Branch Based History Analysis}
The accuracy of the history analysis could be improved significantly.
As explained in \autoref{section:implementation/history_analysis/sorting_tags}, the problem is sorting the tags of a reference project's repository in the correct order without producing huge amounts of changed files between two revisions and slowing down the indexing process.
A better approach would be to start with important branches and find tags on the way back to the root of the commit tree.
Especially for big codebases with many branches, it may be important to filter branches, which are only used to implement features and are later on merged with a release branch.
Since this heavily depends on the policy of the individual reference projects, manual inspection would be required or an approach to automate the filtering has to be developed.

\section{Reducing Amount of Accidental Clones (AC)}
With the current state of the approach, findings have to be inspected manually for violations, which sometimes can be a very time consuming process, since the actual origin of the code may not be part of the reference systems.
Also, many of the found matches are not relevant, since they are either accidental clones and are not actually copied, or the code in question does not cause any licensing issues.
Results and insights emerging from the development of the tool presented in this work show that solving those problems is difficult.
This mostly is caused by the peculiarity and individuality of reference projects and their licensing models.

As mentioned in \autoref{section:evaluation/detecting_infringements} one of the reasons for accidental clones are getters and setters.
Removing those during the normalization process would probably reduce the amount of accidental clones.

Another source of accidental clones are declarations done for API compliance outside of methods.
Testings done in \autoref{chapter:evaluation} showed, that most copied code is located inside the body of methods on both sides, the reference and target system.
Concentrating the comparison of code on code blocks such as methods, structs, enumerations or other language specific constructs, could reduce the amount of such false positives.

Clients could also blacklist matches, which are irrelevant.
Matches which are blacklisted by many clients could then be sorted out on the server side and excluded from results and the filter.

\section{Hash Filter Improvements}
The original idea for a Bloom filter is already more than 40 years old \cite{bloom1970filter}.
There has been a lot of research to improve the filters performance and reduce its size.
Tulsiani et al. found out that \glqq making a sparse Bloom filter using 48 bits per element but only 3 hash functions, one can compress the result down to less than 16 bits per item with high probability and decrease the false positive probability by roughly a factor of 2\grqq \ \cite{tulsiani2013probability}.
Compared to the example calculation done in \autoref{section:implementation/hash_filter}, a filter containing one billion hashes would be about 2 GB, reducing the size by 17\%.
The work of Fan et al. proposes a filter structure called Cuckoo-Filter, which has a better performance when the filter contains many items and needs a low false positive probability.
This would fit the requirements for a filter in this work.