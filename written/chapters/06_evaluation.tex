% !TeX root = ../main.tex

\chapter{Evaluation}\label{chapter:evaluation}
To verify the capability of the approach proposed in this work, the prototypical implementation was tested.
The idea is to find out, whether the approach is capable of detecting copied code in a codebase and, by that, unveil license infringements.
For that purpose, 2000 GPL licensed projects were indexed using the prototypical implementation of the tool.
After that, other open source projects, which are licensed not compatible with GPL and therefore should not contain any GPL licensed code indexed before, where analyzed and emerging matches categorized.

This chapter describes the test setup evaluates timings, database sizes and findings.

\section{Test Setup}
As a source for the projects to index, GitHub was chosen.
GitHub's REST API was used to generate a list of 1.000 GPL licensed Java and 1.000 GPL licensed C or C++ projects.
All of the projects were checked out locally resulting in sizes as shown in \autoref{table:locs}.

Indexing those 2.000 projects including their history took 15h on a consumer laptop with 16 GB of RAM and an Intel i7 2.6 GHz CPU with 4 cores and hyper-threading.
The indexing was done independently for Java and C/C++ generating two databases.
The resulting database sizes are 4 GB for Java and 33 GB for C/C++, the bloom filter's sizes are 55 MB and 147 MB for Java and C/C++.

Inspection of several indexed projects showed, the huge difference for the sizes can be justified in the amount of actual code in the repositories.
It seems like Java projects contain more images, binary files or other resources, compared to the C/C++ projects.
Also, the resulting count of chunks and corresponding amount of hashes indicates a lot of duplicated chunks and leads to the assumption that the increase of hashes is asymptotic relative to the growth of chunks.

\begin{table}[ht]
	\centering
	\begin{tabular}{l|rrrrr}
		& \textbf{Scanned Files} & \textbf{Lines of Code} & \textbf{Total size} & \textbf{Hashes} & \textbf{Chunks} \\ 
		\hline 
		Java & 836.555 & 57.834.764 & 94 GB & 23.773.065 & 35.069.633 \\
		C/C++ & 1.023.092 & 380.338.327 & 102 GB & 61.349.163 & 221.315.454 \\ 
	\end{tabular}
	\caption{Sizes of the 2.000 indexed projects}\label{table:locs}
\end{table}

After two weeks, the index was updated for both Java and C/C++ as described in \ref{section:implementation/history_analysis/update}.
Approximately one third of the reference projects contained one or more new tags which were indexed.
The update took a total of one hour including calculation of the bloom filter.
This shows that incremental updates of the index are worth the effort of implementation compared to re-indexing the reference projects from scratch.

After indexing those projects, several other projects, licensed not compatible with GPL, where analyzed for matches using the approach described in section \ref{section:implementation/finding_matches}.
\autoref{table:target_projects} shows the chosen target projects, their license and the lines of code of files written in the corresponding language.

\todo{Exclude third party and generated code}

\begin{table}[ht]
	\centering
	\begin{tabular}{l|llr}
		& \textbf{Name} & \textbf{License} & \textbf{Lines of Code} \\
		\hline 
		\parbox[t]{2mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{JAVA}}} 
		& IntelliJ IDEA & Apache 2.0 & 3.500.000 \\
		& Eclipse JDT Core & Eclipse Public License & 1.459.000 \\
		& Elasticsearch & Apache 2.0 & 711.000 \\
		& Eclipse JDT UI & Eclipse Public License & 685.000 \\
		& Facebook Buck & Apache 2.0 & 597.000 \\
		& Teamscale & Closed Source & 480.000 \\
		& Spring Boot & Apache 2.0 & 223.000 \\
		& Openfire & Apache 2.0 & 200.000 \\
		& Killbill & Apache 2.0 & 150.000 \\
		& JabRef & MIT & 125.000 \\
		\hline 
		\parbox[t]{2mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{C/C++}}} 
		& Chromium & BSD License 2.0 & 4.651.000 \\
		& ArangoDB & Apache 2.0 & 4.855.000 \\
		& Tensorflow & Apache 2.0 & 662.000 \\
		& Apple Swift & Apache 2.0 & 520.000 \\
		& Mesos & Apache 2.0 & 309.000 \\
		& Apache httpd & Apache 2.0 & 214.000 \\
		& RethinkDB & Apache 2.0 & 201.000 \\
		& Tesseract & Apache 2.0 & 147.000 \\
		& Bitcoin & MIT & 119.000 \\
		& Electron & MIT & 67.000 \\
	\end{tabular}
	\caption{Target projects used for testing, their license and LOCs in the corresponding language}\label{table:target_projects}
\end{table}


\section{Hash Filter Performance}\label{section:evaluation/findings/hash_filter_performance}
The bloom filter can return false positives, thus, matches are expected even if no code has been copied from a reference project into the target project for large enough projects.
Those matches - called false positive filter matches (FPFM) \todo{FPFM needed?} in the remainder of this work - are expected to occur once for roughly every 10.000 statements in the target project, since the probability for a false positive of the bloom filter is calculated to be 0,01\%. 

Table \autoref{table:unfiltered_findings} lists results regarding amount of analyzed chunks as well as false and true positive matches.
Columns are defined as follows:
\begin{description}
	\item[Target System] The name of the target system
	\item[Files] Number of files of the target project, which match the corresponding language (Java or C/C++) and are therefore analyzed.
	\item[Chunks] Resulting amount of chunks extracted from the target system.
	\item[FPFM] False Positive Filter Matches as described above.
	\item[True Positives] Amount of actual matches, which could be found in the index and their relative amount in comparison to all chunks extracted from the target system.
	\item[Requests] Amount of requested match details. Note here that files with less than two matches are ignored as described in section \ref{section:implementation/finding_matches}.
\end{description}
The first two colums called \textit{Files} and \textit{Chunks} show the amount of scanned files and the resulting number of chunks extracted from the target system.
The FPFMs and the percentage in relation to all matches are shown in the third column.
The last column shows the amount of actual matches, which could be found in the index and their relative amount in comparison to all chunks extracted from the target system.
\todo{more details on match distribution and assertions about significance}

\begin{table}[ht]
	\centering
	\begin{tabular}{l|lrrrrr}
		 & \textbf{Target System} & \textbf{Files} & \textbf{Chunks} & \textbf{FPFM} & \textbf{True Positives} & \textbf{Requests} \\ 
		\hline 
		\parbox[t]{2mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{JAVA}}} 
		& IntelliJ IDEA & 35.398 & 1.570.170 & 183 & 8.134 (0,5\%) & 8.035 \\
		& Eclipse JDT Core & 1.829 & 267.130 & 29 & 510 (0,2\%) & 509 \\
		& Elasticsearch & 5.560 & 414.448 & 45 & 1.105 (0,3\%) & 1.004 \\
		& Eclipse JDT UI & 2.736 & 274.040 & 29 & 482 (0,2\%) & 444 \\
		& Buck & 5.150 & 261.605 & 21 & 575 (0,2\%) & 548 \\
		& Teamscale & 4.344 & 145.029 & 20 & 378 (0,3\%) & 365 \\
		& Spring Boot & 3.756 & 94.935 & 11 & 1.010 (1,1\%) & 967 \\
		& Openfire & 1.572 & 121.775 & 15 & 2.106 (1,7\%) & 2.084 \\
		& Killbill & 1.477 & 79.118 & 13 & 550 (0,7\%) & 539 \\
		& JabRef & 1.389 & 72.052 & 5 & 204 (0,3\%) & 183 \\
		\hline 
		\parbox[t]{2mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{C/C++}}} 
		& Chromium & 14.241 & 364.126 & 53 & 16.951 (4,7\%) & 16.912 \\
		& ArangoDB & 1.096 & 135.291 & 8 & 571 (0,4\%) & 569 \\
		& Tensorflow & 1.207 & 50.382 & 3 & 59 (0,2\%) & 56 \\
		& Apple Swift & 848 & 56.386 & 11 & 14 (0,0\%) & 13 \\
		& Mesos & 341 & 64.090 & 6 & 18 (0,0\%) & 15 \\
		& Apache httpd & 529 & 126.936 & 18 & 949 (0,7\%) & 949 \\
		& RethinkDB & 91 & 8.664 & 0 & 2.906 (33,5\%) & 2.905 \\
		& Tesseract & 559 & 82.946 & 10 & 27 (0,0\%) & 29 \\
		& Bitcoin & 491 & 57.847 & 5 & 97 (0,2\%) & 97 \\
		& Electron & 342 & 6.846 & 1 & 0 (0,0\%) & 0 \\
	\end{tabular}
	\caption{Amount of files and chunks as well as true and false positives caused by the Hash Filter and the resulting requested chunk details}\label{table:unfiltered_findings}
\end{table}

This result shows, that the Hash Filter is reducing the required lookups to a fraction.
Load on the server can be greatly reduced and analysis of a target system's code therefore is a lot quicker.

Still, requesting the locations for each hash passing the Hash Filter individually, causes a lot of requests with high overhead caused by creating single network packets.
Additional authentication on the server may be required to control access further slowing down the analysis.
Instead, sending hashes in batches may be a better solution.

\section{Detection of License Infringements}
As the previous section shows, matches have to be filtered, in order to remove false positives and matches, which are not relevant in the context of license infringement.
Aggregation and filtering is done as described in \autoref{section:implementation/finding_matches}.

To get a better view on the results, each match is manually inspected and grouped into one of the following categories:
\begin{description}
	\item [Accidential Clones (AC)]
		Matches which are similar by \glqq accident\grqq, e.g. generated code, interface implementations, declarations of structs/enums, switch-case blocks, table data especially present in C/C++.
	\item[Otherwise Licensed Code (OLC)]
		Code which is actually copied from the reference project or originates from a common source.
		Attribution is done correcty and therefore, this is not a license infringement.
		Code can also be copied into the reverse direction in the context of this evaluation, since analyzed target projects are licensed in a way, where they can be copied to GPL licensed code with appropriate attribution.
	\item[Copied without Attribution (COA)] 
		Same as otherwise licensed code, but not attributed correctly by the target project.
		This may also be seen as a violation of the license's terms, depending on the license of the source, the length of the copied segment and its similarity.
	\item[License Infringement (LI)]
		Code which has been copied and is violating the license of the reference system.
\end{description}

AC can be seen as a false positive of the approach, whereas OLC detects code, which has been copied, but the license of the code is not violated.
Even though OLC does not violate a license, detection of such code still may be of interest, since using a library instead of copying the code into the codebase can be the better alternative \cite{heinemann2012effective}.
In general, those two cases are not marking violations.

COA on the other hand, marks a violation of the license, because the code has not been attributed correctly, but the significance of such a violation may not be very high.
Copied code categorized as LI may even be attributed correctly, but still is a violation of the copied code's license.

\subsection{Counting Matches}
A scanned target system file can have multiple matches for a chunk, since clones also exist between and within reference systems.
Therefore, in the following evaluation, matches are counted on the target file and only the most relevant concatenated matches of the reference files are regarded.
\autoref{table:scan_results} shows the results of manually categorizing each match.
A match is a relation between a file of the target system to the best match of a file in a reference system, indicating a high similarity.

If a target file has multiple sections of copied code, which \textit{does not overlap} and is from multiple other files (e.g. common in utils code), each match in the target file is counted once.
Figure \ref{fig:match_counting} shows an example, where multiple sections of code are copied from \texttt{Reference File 1} to the \texttt{Target File}.
This is counted as one match.
There is also one coherent block of code copied from \texttt{Reference File 2} to the \texttt{Target File}, which is also counted as a match.
The resulting total count of matches for the \texttt{Target File} in this example is two.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/match_counting.pdf}
	\caption{Illustration of match counting}\label{fig:match_counting}
\end{figure}

\subsection{Results}
The results in table \ref{table:scan_results} are twofold.
On the one hand, it shows that finding license infringements is possible.
On the other hand, the amount of overall detected matches prevails the amount of relevant matches by far.
This is due to many accidental clones, which are caused by e.g. interface implementations or tables, as well as the fact that most licenses of target projects allow copying of code into a codebase licensed under GPL.
Therefore, most of the hits for OLC are caused by code of the target project being copied into the reference project.

\begin{table}[ht]
	\centering
	\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
	\begin{tabular}{l | l rrrr}
		& \textbf{Name} & \textbf{AC} & \textbf{OLC} & \textbf{COA} & \textbf{LI} \\
		\hline 
		\parbox[t]{2mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{JAVA}}} 
		& IntelliJ IDEA & 86 & 21 & 11 & 9 \\
		& Eclipse JDT Core & 17 & 8 & 0 & 0 \\
		& Elasticsearch & 13 & 3 & 4 & 3 \\
		& Eclipse JDT UI & 3 & 19 & 0 & 0 \\
		& Facebook Buck & 11 & 7 & 0 & 0  \\
		& Teamscale & 12 & 3 & 1 & 2 \\
		& Spring Boot & 19 & 6 & 0 & 0 \\
		& Openfire & 20 & 9 & 5 & 0 \\
		& Killbill & 20 & 2 & 0 & 0 \\
		& JabRef & 3 & 2 & 0 & 1 \\
		\hline 
		\parbox[t]{2mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{C/C++}}} 
		& Chromium & 16 & 429 & 0 & 0 \\
		& ArangoDB & 1 & 5 & 0 & 0 \\
		& Tensorflow & 2 & 0 & 0 & 0 \\
		& Apple Swift & 2 & 0 & 0 & 0 \\
		& Mesos & 1 & 0 & 0 & 0 \\
		& Apache httpd & 5 & 13 & 0 & 0 \\
		& RethinkDB & 0 & 29 & 0 & 0 \\
		& Tesseract & 2 & 1 & 0 & 0 \\
		& Bitcoin & 0 & 6 & 0 & 0 \\
		& Electron & 0 & 0 & 0 & 0 \\
	\end{tabular}
	\caption{Target projects and categorized matches}\label{table:scan_results}
\end{table}

In general, the findings for Java has more relevant hits, since no LI nor COA were found in C/C++.
\todo{Why is that?}

In many cases, this is not done correctly, as manual inspection showed.
The violation of the license is still given in such a case, although it may not be as critical as e.g. with GPL licensed code.

\todo[inline]{Determination of source not easy -> code origin analysis}

\todo[inline]{Often reference project also have violations (COA), override original header with own gpl}

\todo[inline]{attributed vs non-attributed (= violation) statistics}

\todo[inline]{Match often not origin, but gives hints that something has not been attributed correctly}

\todo[inline]{Chromium: mostly hits from GPL fork}
\todo[inline]{Rethink: mostly hits from rapidjson library}

\section{Special cases}
Some of the findings are rather special and can not be categorized precisely.
This section lists some of those cases and suggestions on how to handle those, if possible.

\begin{itemize}
	\item Overture -> Eclipse
	\item reverse copied code, e.g. Apache code in GPL
	\item Interface implementation quite similar: Java list implementation, equals code, tables in c
	\item Structs/Enums for API (e.g. Facebook Buck/ConstantTags.java)
	\item Decision if accidential or copied without attribution often not easy (facebook Buck/MemoryHandler), COmment can give hints: Both sides same comments, probably copied from somewhere else
\end{itemize}

%\section{Database and Filter Sizes}
%The space $m$ in bits required to store $n$ values with the optimal number of hash functions can be estimated by \todo{quelle?}
%\begin{equation}\label{equ:bloom_probability} 
%m = -\frac{n\cdot \ln(p)}{(\ln2)^{2}}
%\end{equation}
%with $p$ being the probability for a false positive.
%For one billion hashes and a probability of 0.01\%, which roughly means one false positive for every 10000 lines of code, about 2,4 GB are needed.
%A simple hash table would need at least 16 GB for the same amount of data.
%TODO chunksize? => realtion to size of database

\section{Confidentiality}
The purpose of the tool proposed in this work is to uncover licensing issues and therefore may be used for closed source target systems.
One key issue for companies is confidentiality regarding own source code.
Uploading code to a server maintained externally to the company may not be conform with the company's data privacy policies.

The approach presented in this work is only sending hashes which encapsulate information relevant for identifying chunks.
In theory, this should restrict flow of information about the source code to a minimum and conclusions about the code can not be made.
However, when the code for a hash is known on the server side, i.e. a match for a hash is found, the server knows the code for the hash on the client.
If enough matches are found, the server may be able to reconstruct parts of the source code.
Still, information about the source code leaking from the client to the server is very limited.
\todo{Is this relevant? remove?}
\todo[inline]{further improve confidentiality: parital hashes}